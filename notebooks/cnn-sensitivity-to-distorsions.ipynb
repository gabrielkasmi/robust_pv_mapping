{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does the CNN reacts to distorsions in the lower scales\n",
    "\n",
    "Analysis with the ScatNet and the behavior on the dataset with varying AC showed that the model no longer sees PV panels and the the features leading to that are located in the low scales and are poorly transferable from one dataset to the other. \n",
    "\n",
    "Here, we verify this assumption by:\n",
    "- Studying the correlation between the probability shift and the distance between the images\n",
    "- See how the model responds by using the WCAM and see if a shift in the WCAM (interpreted as an inconsistency) is correlated with a shift in predicted probability\n",
    "\n",
    "The shift in prob is what we observe from the model. The WCAM helps us understand whether it changed its behavior and the distance between images helps us establish whether there is something that actually changed. \n",
    "\n",
    "We use the Euclidean norm and the SSIM metrics to compute the image (dis)similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image\n",
    "import json\n",
    "from src import utils\n",
    "import torchvision\n",
    "import pywt\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import torch\n",
    "from spectral_sobol.torch_explainer import WaveletSobol\n",
    "import scipy\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a bunch of images from the test set instances\n",
    "# get the name of the test images, same accross Google and IGN\n",
    "images_list=json.load(open('images_lists.json'))['test'] \n",
    "\n",
    "dataset_dir='path/to/bdappv/data'\n",
    "n_samples=len(images_list)\n",
    "print(n_samples)\n",
    "\n",
    "np.random.seed(42)\n",
    "images_subset=np.random.choice(images_list,n_samples)\n",
    "\n",
    "# load the images\n",
    "images_google=[\n",
    "    Image.open(\n",
    "        os.path.join(dataset_dir.format('google'),im_name) \n",
    "    ).convert('RGB') for im_name in images_subset\n",
    "]\n",
    "\n",
    "images_ign=[\n",
    "    Image.open(\n",
    "        os.path.join(dataset_dir.format('ign'),im_name) \n",
    "    ).convert('RGB') for im_name in images_subset\n",
    "]\n",
    "\n",
    "# rescale them\n",
    "images_google=[\n",
    "    torchvision.transforms.Resize(200)(im) for im in images_google\n",
    "]\n",
    "images_ign=[\n",
    "    torchvision.transforms.CenterCrop(200)(im) for im in images_ign\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image and lower scale similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_approximation_coefficients(image, levels=3):\n",
    "    \"\"\"\n",
    "    returns the approximation coefficients at the specified scale\n",
    "    \"\"\"\n",
    "    if not isinstance(image,np.ndarray):\n",
    "        image=np.array(image)\n",
    "\n",
    "    input=image # take the approximation coefficients\n",
    "                # of the latest level as the input\n",
    "                # of the current level\n",
    "    for _ in range(levels):\n",
    "        i_A, (_, _, _)=pywt.dwt2(input, \"haar\")\n",
    "        input=i_A\n",
    "        \n",
    "    approx=i_A # return the last approx coefficients\n",
    "\n",
    "    return approx\n",
    "\n",
    "def compute_similarity(im_a, im_b, metric=\"ssim\"):\n",
    "    \"\"\"\n",
    "    computes the similarity between two images\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(im_a,np.ndarray):\n",
    "        im_a=np.array(im_a)\n",
    "\n",
    "    if not isinstance(im_b, np.ndarray):\n",
    "        im_b=np.array(im_b)\n",
    "\n",
    "    if metric==\"euclidean\":\n",
    "        # Calculate squared difference between corresponding pixel values\n",
    "        squared_diff = (im_a - im_b) ** 2\n",
    "\n",
    "        # Compute the mean of squared differences\n",
    "        mean_squared_diff = np.mean(squared_diff)\n",
    "\n",
    "        # Take the square root to obtain the RMSE\n",
    "        return np.sqrt(mean_squared_diff)\n",
    "    \n",
    "    elif metric==\"ssim\":\n",
    "        return ssim(im_a, im_b, \n",
    "                    data_range=im_b.max() - im_b.min())\n",
    "    \n",
    "similarities={\n",
    "    'ssim':[],\n",
    "    'euclidean':[]\n",
    "}\n",
    "\n",
    "for im_a, im_b in zip(images_google, images_ign):\n",
    "\n",
    "    im_a=im_a.convert('L')\n",
    "    im_b=im_b.convert('L')\n",
    "\n",
    "    # compute the approximation coefficients\n",
    "    approx_a=get_approximation_coefficients(im_a)\n",
    "    approx_b=get_approximation_coefficients(im_b)\n",
    "    \n",
    "    similarities['ssim'].append(\n",
    "        compute_similarity(approx_a,approx_b,metric=\"ssim\")\n",
    "        #compute_similarity(im_a,im_b,metric=\"ssim\")\n",
    "\n",
    "    )\n",
    "    similarities['euclidean'].append(\n",
    "        compute_similarity(approx_a,approx_b,metric=\"euclidean\")\n",
    "        #compute_similarity(im_a,im_b,metric=\"euclidean\")\n",
    "    )\n",
    "\n",
    "# plot the similarities between the images\n",
    "fig, ax = plt.subplots(1,2)\n",
    "\n",
    "ax[0].hist(similarities['ssim'])\n",
    "ax[1].hist(similarities['euclidean'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the predicted probabilities\n",
    "BASELINE = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean = (0.485, 0.456, 0.406), std = (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "device='device'\n",
    "model=torch.load('path/to/models').to(device)\n",
    "\n",
    "x_ign=torch.stack([\n",
    "    BASELINE(x) for x in images_ign\n",
    "])\n",
    "\n",
    "x_google=torch.stack([\n",
    "    BASELINE(x) for x in images_google\n",
    "])\n",
    "\n",
    "\n",
    "def compute_probabilities(model, x, batch_size=256,label=1):\n",
    "    \"\"\"computes the predicted probabilities\"\"\"\n",
    "\n",
    "    device=next(model.parameters()).device\n",
    "\n",
    "    y=np.empty(x.shape[0])\n",
    "    nb_batch=int(np.ceil(x.shape[0]/batch_size))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_index in range(nb_batch):\n",
    "\n",
    "            start_index=batch_index*batch_size\n",
    "            end_index=min(\n",
    "                x.shape[0], (batch_index+1)*batch_size\n",
    "            )\n",
    "            batch_x=x[start_index:end_index,:,:,:].to(device)\n",
    "            batch_y=model(batch_x)\n",
    "            # softmax the preds\n",
    "            batch_y=torch.softmax(batch_y,dim=1)\n",
    "            batch_y=batch_y[:,label].cpu().detach().numpy()\n",
    "            y[start_index:end_index]=batch_y\n",
    "\n",
    "    return y\n",
    "\n",
    "ign_probs=compute_probabilities(model,x_ign)\n",
    "google_probs=compute_probabilities(model,x_google)\n",
    "delta_probs=np.abs(ign_probs-google_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift_index=np.argwhere(delta_probs>.5)\n",
    "remaining_index=np.argwhere(delta_probs<=0.5)\n",
    "\n",
    "for metric in ['ssim', \"euclidean\"]:\n",
    "\n",
    "\n",
    "    print(metric)\n",
    "\n",
    "    mean_shift=np.mean(np.array(similarities[metric])[shift_index])\n",
    "    mean_stable=np.nanmean(np.array(similarities[metric])[remaining_index])\n",
    "\n",
    "    r, pval=scipy.stats.mstats.pearsonr(similarities[metric], delta_probs)\n",
    "   \n",
    "\n",
    "    print(mean_shift,mean_stable)\n",
    "    \n",
    "    print(\n",
    "        'Correlation coefficient: {:0.4f} ({:0.4f})'.format(r,pval)\n",
    "    )\n",
    "\n",
    "    plt.scatter(similarities[metric],delta_probs)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment: images for which the probability shifts the most have a lower similarity than the images for which it does not change (on average). Shifts are very concentrated: either near 0 or near 1 (reflects the disappearance of a factor). SSIM of 0 indicates no similarity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WCAM\n",
    "\n",
    "We plot a set of WCAMs and compute for each of the predictions the (dis)similarity between them, according to two metrics: the scale consistency and the raw similarity (measured by the euclidean distance between the WCAMs). These metrics indicate how similar the scale decompositions are between the two images. We see how it correlates with the shift in prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers\n",
    "\n",
    "def rgb_to_wavelet_array(image, wavelet='haar', level=3):\n",
    "    # Convert PIL image to NumPy array\n",
    "    img_array = np.array(image.convert('L'))\n",
    "\n",
    "    # Compute wavelet transform for each channel\n",
    "    c = pywt.wavedec2(img_array, wavelet, level=level)     \n",
    "    # normalize each coefficient array independently for better visibility\n",
    "    c[0] /= np.abs(c[0]).max()\n",
    "    for detail_level in range(level):\n",
    "        c[detail_level + 1] = [d/np.abs(d).max() for d in c[detail_level + 1]]\n",
    "    arr, _ = pywt.coeffs_to_array(c)\n",
    "\n",
    "    \n",
    "    return arr\n",
    "\n",
    "def plot_wcam(ax, image, wcam, levels, vmin = None, vmax = None):\n",
    "    \"\"\"\n",
    "    plts the wcam\n",
    "    \"\"\"\n",
    "\n",
    "    def logplot(x):\n",
    "        return np.log(1 + x)\n",
    "    \n",
    "    size = image.size[0]\n",
    "    # compute the wavelet transform\n",
    "    wt = rgb_to_wavelet_array(image,level = levels)\n",
    "    \n",
    "    # plots\n",
    "    ax.imshow(wt, cmap = 'gray')\n",
    "\n",
    "    vmin = logplot(vmin) if vmin is not None else vmin\n",
    "    vmax = logplot(vmax) if vmax is not None else vmax\n",
    "\n",
    "    #im = ax.imshow(1 + logplot(wcam), cmap = \"hot\", alpha = 0.5, vmin = vmin, vmax = vmax)\n",
    "\n",
    "    minlog = np.min(logplot(wcam))\n",
    "    im = ax.imshow(minlog  + logplot(wcam), cmap = \"hot\", alpha = 0.7, vmin = vmin, vmax = vmax)\n",
    "\n",
    "    ax.axis('off')\n",
    "    utils.add_lines(size, levels, ax)\n",
    "\n",
    "    #cbar = plt.colorbar(im, ax = ax)\n",
    "    #cbar.ax.tick_params(labelsize=10)\n",
    "\n",
    "    return None\n",
    "\n",
    "def compute_scale_embedding(wcam, levels=3, grid_size = 28):\n",
    "    \"\"\"\n",
    "    computes the scale embedding of the prediction \n",
    "    from the wcam\n",
    "    \"\"\"\n",
    "    scale_embedding = []\n",
    "\n",
    "    # split the wcam to get the different scales\n",
    "    # and compute the importance of each component\n",
    "    # from the outermost to the innermost level\n",
    "    for level in range(levels + 1):\n",
    "        end, start = grid_size // (2 ** level), grid_size // (2 ** (level + 1))\n",
    "        if start == 1:\n",
    "            start = 0\n",
    "\n",
    "        if level == levels: # approximation coefficient\n",
    "            a = wcam[:end, end]\n",
    "            scale_embedding.append(sum(a.flatten()))\n",
    "        else:\n",
    "            h = wcam[:start, start:end]\n",
    "            v = wcam[start:end, :start]\n",
    "            d = wcam[start:end, start:end]\n",
    "\n",
    "            scale_embedding.append(sum(h.flatten()))\n",
    "            scale_embedding.append(sum(v.flatten()))\n",
    "            scale_embedding.append(sum(d.flatten()))\n",
    "\n",
    "\n",
    "    # return the scale embedding\n",
    "    scale_embedding = np.array(scale_embedding[::-1]) \n",
    "    scale_embedding /= sum(scale_embedding)\n",
    "\n",
    "    return scale_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantitative evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_size=40\n",
    "\n",
    "# pick the desired model \n",
    "wavelet = WaveletSobol(model, grid_size = grid_size, nb_design= 4, \\\n",
    "                       batch_size = 128, opt = {'size' : grid_size, \"approximation\": True})\n",
    "\n",
    "wcams = {}\n",
    "\n",
    "np.random.seed(42)\n",
    "num_wcam=64\n",
    "sample_indices=np.random.choice(n_samples,num_wcam)\n",
    "\n",
    "subset_ign=[images_ign[k] for k in sample_indices]\n",
    "subset_google=[images_google[k] for k in sample_indices]\n",
    "\n",
    "# inputs to the wcam\n",
    "x_ign=torch.stack([\n",
    "    BASELINE(x) for x in subset_ign\n",
    "])\n",
    "y_ign=(ign_probs[sample_indices]>0.44).astype(int)\n",
    "\n",
    "x_google=torch.stack([\n",
    "    BASELINE(x) for x in subset_google\n",
    "])\n",
    "y_google=(google_probs[sample_indices]>0.44).astype(int)\n",
    "\n",
    "\n",
    "wcams_ign=wavelet(x_ign,y_ign)\n",
    "wcams_google=wavelet(x_google,y_google)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the embeddings in a matrix\n",
    "# compute the rmses on the fly\n",
    "levels=3\n",
    "embeddings=np.zeros((num_wcam,1+levels*3,2))\n",
    "wcams_distance=[]\n",
    "\n",
    "for i in range(num_wcam):\n",
    "\n",
    "    embeddings[i,:,0]=compute_scale_embedding(wcams_ign[i], grid_size=grid_size)\n",
    "    embeddings[i,:,1]=compute_scale_embedding(wcams_google[i], grid_size=grid_size)\n",
    "\n",
    "    wcams_distance.append(\n",
    "        compute_similarity(wcams_ign[i], wcams_google[i], metric=\"euclidean\")\n",
    "    )\n",
    "\n",
    "# get the delta prob corresponding to the selected \n",
    "# images\n",
    "selected_probs=delta_probs[sample_indices]\n",
    "\n",
    "# compute the distance between the embeddings\n",
    "distance=np.linalg.norm(embeddings[:,:,0]-embeddings[:,:,1], axis=1)\n",
    "\n",
    "fig, ax=plt.subplots(1,2)\n",
    "\n",
    "ax[0].scatter(distance,selected_probs)\n",
    "ax[1].scatter(wcams_distance,selected_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the correlation coefficient here between \n",
    "# the distance and the prob, and the wcams and the prob\n",
    "\n",
    "for case, y in zip([\"Scale embedding\", \"WCAM RMSE\"], [distance,wcams_distance]):\n",
    "    r, pval=scipy.stats.mstats.pearsonr(y, selected_probs)\n",
    "    print(\n",
    "        'Correlation coefficient: {:0.4f} ({:0.4f})'.format(r,pval)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index=16\n",
    "\n",
    "\n",
    "fig, ax=plt.subplots(2,2,figsize=(8,8))\n",
    "\n",
    "ax[0,1].imshow(subset_ign[index])\n",
    "ax[0,1].set_title('IGN - $\\Delta p$={:0.2f}'.format(selected_probs[index]))\n",
    "ax[0,0].imshow(subset_google[index])\n",
    "ax[0,0].set_title('Google - Prob : {:0.2f}'.format(google_probs[sample_indices[index]]))\n",
    "\n",
    "ax[0,0].axis('off')\n",
    "ax[0,1].axis('off')\n",
    "\n",
    "ax[1,0].set_title('WCAM')\n",
    "ax[1,1].set_title('WCAM')\n",
    "\n",
    "plot_wcam(ax[1,1], subset_ign[index], cv2.resize(wcams_ign[index], (200,200), interpolation=cv2.INTER_LINEAR), levels=3)\n",
    "plot_wcam(ax[1,0], subset_google[index], cv2.resize(wcams_google[index], (200,200), interpolation=cv2.INTER_LINEAR), levels=3)\n",
    "\n",
    "plt.savefig('example_delta_p_{}.pdf'.format(index))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "acquisition",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
