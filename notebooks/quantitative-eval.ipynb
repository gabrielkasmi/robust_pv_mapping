{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark\n",
    "\n",
    "Notebook to reproduce table 2 using pretrained models on Google images.\n",
    "\n",
    "The models all have a ResNet-50 backbone and are the following:\n",
    "\n",
    "- Standard: no particular augmentation beyond crops, resizes and rotations,\n",
    "- AugMix: data augmentations using AugMix\n",
    "- Blurring and noising: applying random noise and blur to improve the robustness against these perturbations.\n",
    "- Target: we train a model on IGN to evaluate the oracle accuracy\n",
    "\n",
    "\n",
    "Our methods are the following:\n",
    "- Blurring : surrogate for the scale removal, requiring only to apply a well-calibrated Gaussian blur on the image\n",
    "- Scale removal: based on the wavelet transform of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from PIL import Image, ImageEnhance\n",
    "import matplotlib.pyplot as plt\n",
    "from src import bdappv, utils, helpers\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "import tqdm\n",
    "import torch\n",
    "import random\n",
    "from functools import reduce \n",
    "import copy\n",
    "from spectral_sobol.torch_explainer import WaveletSobol\n",
    "from torch.nn import functional as F\n",
    "import os\n",
    "import pywt\n",
    "from src import reconstruction\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helpers\n",
    "\n",
    "def rgb_to_wavelet_array(image, wavelet='haar', level=3):\n",
    "    # Convert PIL image to NumPy array\n",
    "    img_array = np.array(image.convert('L'))\n",
    "\n",
    "    # Compute wavelet transform for each channel\n",
    "    c = pywt.wavedec2(img_array, wavelet, level=level)     \n",
    "    # normalize each coefficient array independently for better visibility\n",
    "    c[0] /= np.abs(c[0]).max()\n",
    "    for detail_level in range(level):\n",
    "        c[detail_level + 1] = [d/np.abs(d).max() for d in c[detail_level + 1]]\n",
    "    arr, _ = pywt.coeffs_to_array(c)\n",
    "\n",
    "    \n",
    "    return arr\n",
    "\n",
    "\n",
    "def plot_wcam(ax, image, wcam, levels, vmin = None, vmax = None):\n",
    "    \"\"\"\n",
    "    plts the wcam\n",
    "    \"\"\"\n",
    "\n",
    "    def logplot(x):\n",
    "        return np.log(1 + x)\n",
    "    \n",
    "    size = image.size[0]\n",
    "    # compute the wavelet transform\n",
    "    wt = rgb_to_wavelet_array(image,level = levels)\n",
    "    \n",
    "    # plots\n",
    "    ax.imshow(wt, cmap = 'gray')\n",
    "\n",
    "    vmin = logplot(vmin) if vmin is not None else vmin\n",
    "    vmax = logplot(vmax) if vmax is not None else vmax\n",
    "\n",
    "    #im = ax.imshow(1 + logplot(wcam), cmap = \"hot\", alpha = 0.5, vmin = vmin, vmax = vmax)\n",
    "\n",
    "    minlog = np.min(logplot(wcam))\n",
    "    im = ax.imshow(minlog  + logplot(wcam), cmap = \"hot\", alpha = 0.7, vmin = vmin, vmax = vmax)\n",
    "\n",
    "    ax.axis('off')\n",
    "    utils.add_lines(size, levels, ax)\n",
    "\n",
    "    #cbar = plt.colorbar(im, ax = ax)\n",
    "    #cbar.ax.tick_params(labelsize=10)\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "device = \"cuda\"\n",
    "models_dir = 'models-google'\n",
    "\n",
    "models_name = [m.split('_')[1][:-4] for m in os.listdir(models_dir) if not m == \"weights\"]\n",
    "models = {\n",
    "    m : torch.load(os.path.join(models_dir, \"model_{}.pth\".format(m))) for m in [\"standard\", \"oracle\"]\n",
    "}\n",
    "\n",
    "# set up the classification thresholds\n",
    "thresholds = {}\n",
    "\n",
    "results_dir = \"results-google\"\n",
    "\n",
    "for file in os.listdir(results_dir):\n",
    "    f = json.load(open(os.path.join(results_dir, file)))\n",
    "    name = file.split('_')[1][:-5]\n",
    "    thresholds[name] = f['best_threshold']\n",
    "\n",
    "print(models.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the models (Table 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the test dataset: IGN images\n",
    "\n",
    "images_list = json.load(open(\"data/images_lists.json\"))\n",
    "\n",
    "dataset_dir = \"../../data/bdappv\"\n",
    "batch_size = 512\n",
    "\n",
    "# baseline transforms: no corruptions\n",
    "baseline = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToPILImage(),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(mean = (0.485, 0.456, 0.406), std = (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "datasets = {}\n",
    "\n",
    "for case in ['google', 'ign']:\n",
    "\n",
    "    path = os.path.join(dataset_dir, case)\n",
    "\n",
    "    if case == \"google\":\n",
    "\n",
    "        dataset = bdappv.BDAPPVClassification(path, size = 200, transform=baseline, images_list=images_list[\"test\"], random = False, downsample=200)\n",
    "    else:\n",
    "        dataset = bdappv.BDAPPVClassification(path, size = 200, images_list=images_list[\"test\"], random = False, transform = baseline)\n",
    "\n",
    "    database = DataLoader(dataset, batch_size=batch_size)\n",
    "    datasets[case] = database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some examples\n",
    "\n",
    "fig, ax = plt.subplots(2,4, figsize = (16,12))\n",
    "indices = [10, 30, 128, 42]\n",
    "\n",
    "def NormalizeData(data):\n",
    "    \"\"\"helper to normalize in [0,1] for the plots\"\"\"\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "\n",
    "for i, key in enumerate(datasets.keys()):\n",
    "    # display an example from each dataset\n",
    "\n",
    "    images, labels, _ = next(iter(datasets[key]))\n",
    "\n",
    "    for j, index in enumerate(indices):\n",
    "        example = images[index,:,:,:].numpy().swapaxes(0,2)\n",
    "        ax[i,j].imshow(NormalizeData(example))\n",
    "        ax[i,j].axis('off')\n",
    "\n",
    "fig.tight_layout()\n",
    "# plt.savefig('figures/examples.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "\n",
    "results = {n : {} for n in models.keys()}\n",
    "\n",
    "for name in models.keys():\n",
    "\n",
    "    model = models[name]\n",
    "    threshold = thresholds[name]\n",
    "\n",
    "    print('Evaluating {} .... '.format(name))\n",
    "\n",
    "    for case in datasets.keys():\n",
    "    \n",
    "        results[name][case] = utils.evaluate(model, datasets[case], device, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('results-updated.json', 'w') as f:\n",
    "    json.dump(results, f, cls=helpers.NpEncoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the results if the file already exists\n",
    "results = json.load(open('results-updated.json'))\n",
    "\n",
    "case = 'ign'\n",
    "\n",
    "labels = {\n",
    "    'ign' :    {},\n",
    "    'google' : {}\n",
    "}\n",
    "\n",
    "probs = {\n",
    "    \"ign\"    : {},\n",
    "    'google' : {}\n",
    "}\n",
    "\n",
    "print('---- Main matter table - case : {} ----'.format(case))\n",
    "\n",
    "for name in results.keys():\n",
    "\n",
    "    f1, tp, fp, tn, fn, predictions, probabilities, truth = results[name][case]\n",
    "    s = '{} & {:0.2f} & {} & {} & {} & {}'.format(name, f1, tp, tn, fp, fn) \n",
    "\n",
    "    labels[case][name] = predictions\n",
    "    probs [case][name] = (probabilities, truth)\n",
    "    \n",
    "    print(s)\n",
    "\n",
    "case = 'google'\n",
    "\n",
    "print('\\n ---- Appendix table - case : {} ----'.format(case))\n",
    "\n",
    "for name in results.keys():\n",
    "\n",
    "    f1, tp, fp, tn, fn, predictions, probabilities, truth = results[name][case]\n",
    "    s = '{} & {:0.2f} & {} & {} & {} & {}'.format(name, f1, tp, tn, fp, fn) \n",
    "\n",
    "    labels[case][name] = predictions\n",
    "    probs[case][name] = (probabilities, truth)\n",
    "    \n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentations\n",
    "\n",
    "Plots of the different augmentation strategies benchmarked in the paper. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import transforms\n",
    "policy = torchvision.transforms.AutoAugmentPolicy.IMAGENET\n",
    "\n",
    "\n",
    "from src import transforms\n",
    "dataset_dir = \"../../data/bdappv\"\n",
    "indices = [42, 95, 20]\n",
    "# Load your image (replace 'input_image.jpg' with your image path)\n",
    "input_images = [\n",
    "    Image.open(os.path.join(dataset_dir + '/google/img', os.listdir(os.path.join(dataset_dir, 'google/img'))[index])).convert('RGB') for index in indices\n",
    "    ]\n",
    "\n",
    "transforms_set = [torchvision.transforms.RandAugment(), torchvision.transforms.AutoAugment(policy), transforms.AugMix()] # , transforms.Spectral(), transforms.NoiseAndBlur(sigma_b = 2., sigma_n = 0., deterministic = True)]\n",
    "\n",
    "transforms_set = [transforms.NoiseAndBlur(sigma_b = 2., sigma_n = 0., deterministic = True), transforms.NoiseAndBlur(sigma_b = 2., sigma_n = 40., deterministic = False), transforms.Spectral()]\n",
    "\n",
    "imgs = {}\n",
    "\n",
    "for name, trans in zip([\"Blurring\", \"Blur and Noise\", \"Blurring + WP\"], transforms_set):#, \"Blurring + WP\", \"Blurring\"], transforms_set):\n",
    "\n",
    "    tr = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.CenterCrop(224),\n",
    "        trans\n",
    "        ])\n",
    "\n",
    "    imgs[name] = [tr(input_image) for input_image in input_images]\n",
    "\n",
    "fig, ax = plt.subplots(4,3, figsize = (12,16))\n",
    "plt.rcParams.update({'font.size': 18})\n",
    "\n",
    "def NormalizeData(data):\n",
    "    \"\"\"helper to normalize in [0,1] for the plots\"\"\"\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
    "\n",
    "crop = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.CenterCrop(224)\n",
    "])\n",
    "\n",
    "\n",
    "for i, name in enumerate(list(imgs.keys())):\n",
    "    ax[0,i].imshow(crop(input_images[i]))\n",
    "    ax[0,i].set_title('Original image')\n",
    "    ax[0,i].axis('off')\n",
    "\n",
    "    images = imgs[name]\n",
    "\n",
    "    for j, img in enumerate(images):\n",
    "\n",
    "        ax[i+1,j].imshow(img)\n",
    "        ax[i+1,j].set_title(name)\n",
    "        ax[i+1,j].axis('off')\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.savefig('examples_augmentations_ours.pdf')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsfrance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
